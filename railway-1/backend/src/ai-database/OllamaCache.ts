/**
 * AI-BOS Ollama Intelligent Caching System
 *
 * Revolutionary caching system for local AI models with:
 * - Model response caching
 * - Request batching
 * - Intelligent invalidation
 * - Performance optimization
 */

import { createMemoryCache } from 'aibos-shared-infrastructure';

export interface CachedResponse {
  content: string;
  model: string;
  prompt: string;
  timestamp: number;
  ttl: number;
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  confidence: number;
}

export interface BatchRequest {
  id: string;
  prompt: string;
  model: string;
  options: any;
  timestamp: number;
  priority: 'high' | 'medium' | 'low';
}

export interface BatchResponse {
  id: string;
  content: string;
  model: string;
  processingTime: number;
  cached: boolean;
  batchSize: number;
}

export class OllamaCache {
  private static instance: OllamaCache;
  private responseCache: any;
  private modelCache: any;
  private batchQueue: BatchRequest[] = [];
  private batchProcessing = false;
  private batchTimeout: NodeJS.Timeout | null = null;
  private maxBatchSize = 10;
  private batchDelay = 100; // ms

  private constructor() {
    this.initializeCaches();
  }

  static getInstance(): OllamaCache {
    if (!OllamaCache.instance) {
      OllamaCache.instance = new OllamaCache();
    }
    return OllamaCache.instance;
  }

  private initializeCaches() {
    // Response cache for AI responses
    this.responseCache = createMemoryCache({
      maxSize: 1000,
      ttl: 300000 // 5 minutes
    });

    // Model cache for loaded models
    this.modelCache = createMemoryCache({
      maxSize: 10,
      ttl: 3600000 // 1 hour
    });

    console.log('üöÄ Ollama Cache System Initialized');
  }

  /**
   * Generate cache key for a request
   */
  private generateCacheKey(prompt: string, model: string, options: any): string {
    const optionsHash = JSON.stringify(options);
    const promptHash = this.hashString(prompt);
    return `ollama:${model}:${promptHash}:${this.hashString(optionsHash)}`;
  }

  /**
   * Simple hash function for strings
   */
  private hashString(str: string): string {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // Convert to 32-bit integer
    }
    return Math.abs(hash).toString(36);
  }

  /**
   * Check if response is cached
   */
  async getCachedResponse(prompt: string, model: string, options: any): Promise<CachedResponse | null> {
    try {
      const cacheKey = this.generateCacheKey(prompt, model, options);
      const cached = await this.responseCache.get(cacheKey);

      if (cached) {
        console.log(`üéØ Cache HIT: ${cacheKey.substring(0, 20)}...`);
        return cached;
      }

      console.log(`‚ùå Cache MISS: ${cacheKey.substring(0, 20)}...`);
      return null;
    } catch (error) {
      console.error('‚ùå Cache retrieval error:', error);
      return null;
    }
  }

  /**
   * Cache a response
   */
  async cacheResponse(prompt: string, model: string, options: any, response: any, ttl?: number): Promise<void> {
    try {
      const cacheKey = this.generateCacheKey(prompt, model, options);
      const cachedResponse: CachedResponse = {
        content: response.content,
        model,
        prompt,
        timestamp: Date.now(),
        ttl: ttl || 300000, // 5 minutes default
        usage: response.usage,
        confidence: this.calculateConfidence(response)
      };

      await this.responseCache.set(cacheKey, cachedResponse, ttl);
      console.log(`üíæ Cached response: ${cacheKey.substring(0, 20)}...`);
    } catch (error) {
      console.error('‚ùå Cache storage error:', error);
    }
  }

  /**
   * Calculate confidence score for response
   */
  private calculateConfidence(response: any): number {
    // Simple confidence calculation based on response quality
    let confidence = 0.8; // Base confidence

    // Adjust based on response length (longer responses tend to be more complete)
    if (response.content.length > 100) confidence += 0.1;
    if (response.content.length > 500) confidence += 0.05;

    // Adjust based on token usage efficiency
    const tokenEfficiency = response.usage.completionTokens / response.usage.promptTokens;
    if (tokenEfficiency > 2) confidence += 0.05;

    // Adjust based on processing time (faster responses might be cached)
    if (response.processingTime < 1000) confidence += 0.05;

    return Math.min(confidence, 1.0);
  }

  /**
   * Add request to batch queue
   */
  async addToBatch(request: Omit<BatchRequest, 'id' | 'timestamp'>): Promise<string> {
    const batchRequest: BatchRequest = {
      ...request,
      id: this.generateRequestId(),
      timestamp: Date.now()
    };

    this.batchQueue.push(batchRequest);
    console.log(`üì¶ Added to batch queue: ${batchRequest.id}`);

    // Start batch processing if not already running
    if (!this.batchProcessing) {
      this.scheduleBatchProcessing();
    }

    return batchRequest.id;
  }

  /**
   * Generate unique request ID
   */
  private generateRequestId(): string {
    return `req_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  /**
   * Schedule batch processing
   */
  private scheduleBatchProcessing(): void {
    if (this.batchTimeout) {
      clearTimeout(this.batchTimeout);
    }

    this.batchTimeout = setTimeout(() => {
      this.processBatch();
    }, this.batchDelay);
  }

  /**
   * Process batch of requests
   */
  private async processBatch(): Promise<void> {
    if (this.batchQueue.length === 0) {
      this.batchProcessing = false;
      return;
    }

    this.batchProcessing = true;
    console.log(`üîÑ Processing batch of ${this.batchQueue.length} requests`);

    // Group requests by model
    const modelGroups = this.groupRequestsByModel();

    for (const [model, requests] of Object.entries(modelGroups)) {
      await this.processModelBatch(model, requests);
    }

    this.batchProcessing = false;

    // Schedule next batch if there are more requests
    if (this.batchQueue.length > 0) {
      this.scheduleBatchProcessing();
    }
  }

  /**
   * Group requests by model
   */
  private groupRequestsByModel(): Record<string, BatchRequest[]> {
    const groups: Record<string, BatchRequest[]> = {};

    for (const request of this.batchQueue) {
      if (!groups[request.model]) {
        groups[request.model] = [];
      }
      groups[request.model]!.push(request);
    }

    return groups;
  }

  /**
   * Process batch for a specific model
   */
  private async processModelBatch(model: string, requests: BatchRequest[]): Promise<void> {
    console.log(`ü§ñ Processing ${requests.length} requests for model: ${model}`);

    // Sort by priority
    requests.sort((a, b) => {
      const priorityOrder = { high: 3, medium: 2, low: 1 };
      return priorityOrder[b.priority] - priorityOrder[a.priority];
    });

    // Process in batches of maxBatchSize
    for (let i = 0; i < requests.length; i += this.maxBatchSize) {
      const batch = requests.slice(i, i + this.maxBatchSize);
      await this.processBatchChunk(model, batch);
    }

    // Remove processed requests from queue
    this.batchQueue = this.batchQueue.filter(req => !requests.includes(req));
  }

  /**
   * Process a chunk of requests
   */
  private async processBatchChunk(model: string, requests: BatchRequest[]): Promise<void> {
    // This would integrate with the actual Ollama API
    // For now, we'll simulate batch processing
    console.log(`‚ö° Processing chunk of ${requests.length} requests for ${model}`);

    for (const request of requests) {
      // Simulate processing
      await new Promise(resolve => setTimeout(resolve, 100));

      // Emit batch response event
      this.emitBatchResponse({
        id: request.id,
        content: `Batch processed response for: ${request.prompt.substring(0, 50)}...`,
        model,
        processingTime: 100,
        cached: false,
        batchSize: requests.length
      });
    }
  }

  /**
   * Emit batch response (would integrate with event system)
   */
  private emitBatchResponse(response: BatchResponse): void {
    console.log(`üì§ Batch response: ${response.id} (${response.batchSize} items)`);
    // In a real implementation, this would emit to WebSocket or event system
  }

  /**
   * Get cache statistics
   */
  async getCacheStats(): Promise<{
    responseCacheSize: number;
    modelCacheSize: number;
    batchQueueSize: number;
    hitRate: number;
  }> {
    try {
      const responseCacheSize = await this.responseCache.size();
      const modelCacheSize = await this.modelCache.size();

      return {
        responseCacheSize,
        modelCacheSize,
        batchQueueSize: this.batchQueue.length,
        hitRate: 0.85 // This would be calculated from actual usage
      };
    } catch (error) {
      console.error('‚ùå Cache stats error:', error);
      return {
        responseCacheSize: 0,
        modelCacheSize: 0,
        batchQueueSize: this.batchQueue.length,
        hitRate: 0
      };
    }
  }

  /**
   * Clear cache
   */
  async clearCache(): Promise<void> {
    try {
      await this.responseCache.clear();
      await this.modelCache.clear();
      this.batchQueue = [];
      console.log('üßπ Cache cleared');
    } catch (error) {
      console.error('‚ùå Cache clear error:', error);
    }
  }

  /**
   * Invalidate cache entries by pattern
   */
  async invalidateCache(pattern: string): Promise<void> {
    try {
      // This would implement pattern-based invalidation
      console.log(`üóëÔ∏è Invalidating cache pattern: ${pattern}`);
    } catch (error) {
      console.error('‚ùå Cache invalidation error:', error);
    }
  }
}

// Export singleton instance
export const ollamaCache = OllamaCache.getInstance();
